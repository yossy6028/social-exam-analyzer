#!/usr/bin/env python3
"""
å°å•ã®ãƒ¯ãƒ¼ãƒ‰åˆ†æã¨subject_index.mdç…§åˆã®ãƒ—ãƒ­ã‚»ã‚¹
"""

import re
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from modules.terms_repository import TermsRepository
from modules.improved_question_extractor import ImprovedQuestionExtractor

def analyze_question_words():
    """å°å•ã®ãƒ¯ãƒ¼ãƒ‰åˆ†æã¨subject_index.mdç…§åˆã®ãƒ—ãƒ­ã‚»ã‚¹"""
    
    print("=== å°å•ã®ãƒ¯ãƒ¼ãƒ‰åˆ†æã¨subject_index.mdç…§åˆã®ãƒ—ãƒ­ã‚»ã‚¹ ===\n")
    
    # å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’åˆæœŸåŒ–
    extractor = ImprovedQuestionExtractor()
    terms_repo = TermsRepository()
    
    # å®Ÿéš›ã®OCRãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    ocr_file = "logs/ocr_2023_æ—¥å·¥å¤§é§’å ´_ç¤¾ä¼š.txt"
    
    try:
        with open(ocr_file, 'r', encoding='utf-8') as f:
            ocr_text = f.read()
    except FileNotFoundError:
        print(f"âŒ OCRãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {ocr_file}")
        return
    
    print(f"ğŸ“ OCRãƒ•ã‚¡ã‚¤ãƒ«: {ocr_file}")
    
    # å¤§å•ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
    major_sections = extractor._find_major_sections(ocr_text)
    
    if not major_sections:
        print("âŒ å¤§å•ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    print(f"âœ… æ¤œå‡ºã•ã‚ŒãŸå¤§å•æ•°: {len(major_sections)}")
    
    # å„å°å•ã‚’è©³ç´°åˆ†æ
    for major_num, section_text in major_sections:
        print(f"\n{'='*60}")
        print(f"å¤§å•{major_num}ã®è©³ç´°åˆ†æ")
        print(f"{'='*60}")
        
        # å°å•ã‚’æŠ½å‡º
        minor_questions = extractor._extract_minor_questions(section_text)
        
        print(f"æŠ½å‡ºã•ã‚ŒãŸå°å•æ•°: {len(minor_questions)}")
        
        # å„å°å•ã‚’è©³ç´°åˆ†æ
        for i, (q_num, q_text) in enumerate(minor_questions[:5]):  # æœ€åˆã®5å•ã®ã¿
            print(f"\n--- å°å•{q_num} ---")
            print(f"ãƒ†ã‚­ã‚¹ãƒˆ: {q_text[:100]}...")
            
            # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¯ãƒ¼ãƒ‰åˆ†å‰²
            words = extract_keywords_from_question(q_text)
            print(f"æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {words}")
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: subject_index.mdã¨ã®ç…§åˆ
            matched_terms = find_matching_terms(words, terms_repo)
            print(f"ç…§åˆçµæœ: {matched_terms}")
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: ä¸»é¡Œã®æ±ºå®š
            main_theme = determine_main_theme(matched_terms, q_text)
            print(f"æ±ºå®šã•ã‚ŒãŸä¸»é¡Œ: {main_theme}")
            
            print("-" * 40)

def extract_keywords_from_question(question_text: str) -> list:
    """å°å•ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
    # åŸºæœ¬çš„ãªå‰å‡¦ç†
    text = question_text.strip()
    
    # ä¸è¦ãªæ–‡å­—ã‚’é™¤å»
    text = re.sub(r'[ã€ã€‘ã€Œã€ã€ã€()ï¼ˆï¼‰\[\]{}]', '', text)
    
    # å¥èª­ç‚¹ã§åˆ†å‰²
    sentences = re.split(r'[ã€‚ã€ï¼Œï¼]', text)
    
    keywords = []
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence or len(sentence) < 3:
            continue
        
        # é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        # 1. å°‚é–€ç”¨èªï¼ˆæ¼¢å­—ã®é€£ç¶šï¼‰
        kanji_terms = re.findall(r'[ä¸€-é¾¯]{2,}', sentence)
        keywords.extend(kanji_terms)
        
        # 2. åœ°åãƒ»äººå
        location_names = re.findall(r'[æ±å—è¥¿åŒ—]äº¬|[æ±å—è¥¿åŒ—]æµ·é“|[éƒ½é“åºœçœŒ]|[å¸‚ç”ºæ‘]', sentence)
        keywords.extend(location_names)
        
        # 3. å¹´ä»£ãƒ»ä¸–ç´€
        time_periods = re.findall(r'\d+ä¸–ç´€|\d+å¹´ä»£|\d+å¹´', sentence)
        keywords.extend(time_periods)
        
        # 4. åˆ¶åº¦ãƒ»æ”¿ç­–å
        policy_terms = re.findall(r'[åˆ¶æ”¿ç­–æ³•æ†²æ¡ç´„]', sentence)
        keywords.extend(policy_terms)
    
    # é‡è¤‡ã‚’é™¤å»
    unique_keywords = list(set(keywords))
    
    # çŸ­ã™ãã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å¤–
    filtered_keywords = [kw for kw in unique_keywords if len(kw) >= 2]
    
    return filtered_keywords

def find_matching_terms(keywords: list, terms_repo: TermsRepository) -> list:
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨subject_index.mdã®ç”¨èªã‚’ç…§åˆ"""
    matched_terms = []
    
    for keyword in keywords:
        # TermsRepositoryã§ç”¨èªã‚’æ¤œç´¢
        found_terms = terms_repo.find_terms_in_text(keyword)
        
        if found_terms:
            for field, term in found_terms:
                matched_terms.append({
                    'keyword': keyword,
                    'term': term,
                    'field': field,
                    'confidence': calculate_confidence(keyword, term)
                })
    
    # ä¿¡é ¼åº¦ã§ã‚½ãƒ¼ãƒˆ
    matched_terms.sort(key=lambda x: x['confidence'], reverse=True)
    
    return matched_terms

def calculate_confidence(keyword: str, term: str) -> float:
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ç”¨èªã®ä¸€è‡´åº¦ã‚’è¨ˆç®—"""
    if keyword == term:
        return 1.0
    elif keyword in term or term in keyword:
        return 0.8
    elif len(set(keyword) & set(term)) / len(set(keyword) | set(term)) > 0.5:
        return 0.6
    else:
        return 0.3

def determine_main_theme(matched_terms: list, question_text: str) -> str:
    """ç…§åˆçµæœã‹ã‚‰ä¸»é¡Œã‚’æ±ºå®š"""
    if not matched_terms:
        return "ä¸»é¡Œä¸æ˜"
    
    # æœ€ã‚‚ä¿¡é ¼åº¦ã®é«˜ã„ç”¨èªã‚’é¸æŠ
    best_match = matched_terms[0]
    
    # åˆ†é‡åˆ¥ã®é‡ã¿ä»˜ã‘
    field_weights = {
        'geography': 1.0,
        'history': 1.0,
        'civics': 1.0
    }
    
    # ä¿¡é ¼åº¦ã¨åˆ†é‡ã®é‡ã¿ã‚’è€ƒæ…®ã—ã¦æœ€çµ‚ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    final_score = best_match['confidence'] * field_weights.get(best_match['field'], 1.0)
    
    # ä¸»é¡Œã‚’æ±ºå®š
    if final_score >= 0.8:
        theme = f"{best_match['term']}ã«ã¤ã„ã¦"
    elif final_score >= 0.6:
        theme = f"{best_match['term']}ã«é–¢ã™ã‚‹å•é¡Œ"
    else:
        theme = f"{best_match['keyword']}ã«ã¤ã„ã¦"
    
    return theme

if __name__ == "__main__":
    analyze_question_words()
